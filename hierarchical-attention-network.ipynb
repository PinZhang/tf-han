{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.5.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HierarchicalAttentionNetwork(object):\n",
    "    def __init__(self, vocab_size, num_class, embed_dim, hidden_dim):\n",
    "        super(HierarchicalNeuralNetwork, self).__init__()\n",
    "        self._vocab_size = vocab_size\n",
    "        self._num_class = num_class\n",
    "        self._embed_dim = embed_dim\n",
    "        self._hidden_dim = hidden_dim\n",
    "    def _make_graph(self, graph):\n",
    "        with graph.as_default():\n",
    "            words  = tf.placeholder(tf.int32, [None, None], name='words')\n",
    "            length = tf.placeholder(tf.int32, [None], name='length')\n",
    "            labels = tf.placeholder(tf.int32, (), name='labels') \n",
    "            \n",
    "            with tf.variable_scope('embeddings'):\n",
    "                embedding = tf.get_variable('lookup', shape=(self._vocab_size, self._embed_dim), dtype=tf.float32, trainable=True)\n",
    "                embeded   = tf.nn.embedding_lookup(embedding, words)\n",
    "            with tf.variable_scope('words_lstm'):\n",
    "                cell_fw = tf.nn.rnn_cell.GRUCell(num_units=self._hidden_dim)\n",
    "                cell_bw = tf.nn.rnn_cell.GRUCell(num_units=self._hidden_dim)\n",
    "                (outputs_fw, outputs_bw), _ = \\\n",
    "                    tf.nn.bidirectional_dynamic_rnn(cell_fw, cell_bw, embeded, sequence_length=length)\n",
    "            outputs = tf.concat([outputs_fw, outputs_bw], axis=2)\n",
    "            with tf.variable_scope('words_attention'):\n",
    "                hidden = tf.layers.dense(outputs, units=self._hidden_dim * 2, activation=tf.nn.tanh)\n",
    "                attention = tf.layers.dense(outputs, units=1, activation=tf.nn.softmax)\n",
    "            sentence_embedding = tf.expand_dims(outputs * attention, axis=0)\n",
    "            with tf.variable_scope('sentence_lstm'):\n",
    "                cell_fw = tf.nn.rnn_cell.GRUCell(num_units=self._hidden_dim)\n",
    "                cell_bw = tf.nn.rnn_cell.GRUCell(num_units=self._hidden_dim)\n",
    "                (outputs_fw, outputs_bw), _ = \\\n",
    "                    tf.nn.bidirectional_dynamic_rnn(cell_fw, cell_bw, sentence_embedding)\n",
    "            outputs = tf.squeeze(tf.concat([outputs_fw, outputs_bw], axis=2), axis=[0])\n",
    "            with tf.variable_scope('sentence_attention'):\n",
    "                hidden = tf.layers.dense(outputs, units=self._hidden_dim * 2, activation=tf.nn.tanh)\n",
    "                attention = tf.layers.dense(hidden, units=1, activation=tf.nn.softmax)\n",
    "            outputs = outputs * attention\n",
    "            logits = tf.layers.dense(outputs, units=self._num_class, activation=None)\n",
    "            loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels)\n",
    "            training_op = tf.train.AdamOptimizer(learning_rate=0.01).minimize(loss)\n",
    "            return words, length, labels, logits, loss, training_op\n",
    "        def fit(self, words_seq, length_seq, labels_seq, num_epochs=10, model=None):\n",
    "            graph = tf.Graph()\n",
    "            words, length, labels, _, loss, training_op = self._make_graph(graph)\n",
    "            \n",
    "            with tf.Session(graph=graph) as sess:\n",
    "                sess.run(tf.global_variables_initializer())\n",
    "                for epoch in range(num_epochs):\n",
    "                    for words_val, length_val, labels_val in zip(words_seq, length_seq, labels_seq):\n",
    "                        feed_dict = { words : words_val, length : length_val, labels : labels_val }\n",
    "                        sess.run(training_op, feed_dict=feed_dict)\n",
    "                    loss_total = 0\n",
    "                    for words_val, length_val, labels_val in zip(words_seq, length_seq, labels_seq):\n",
    "                        feed_dict = { words : words_val, length : length_val, labels : labels_val }\n",
    "                        loss_val = sess.run(loss, feed_dict=feed_dict)\n",
    "                        loss_total += loss_val\n",
    "                    print('Epoch [%d/%d], Loss: %.3f' % (epoch + 1, num_epochs, loss_total))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
